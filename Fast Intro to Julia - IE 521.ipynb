{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Julia is simple. This is how we write vectors, row vectors and Matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = [1; 2; 3]\n",
    "b = [4 5 6]\n",
    "A = [1 2 3; 4 5 6]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accessing matrices elements, transposing matrices and vector, computing the dot product is the same as in MATLAB. Also, the LinearAlgebra Package comes handy for some functions. Let us first learn how to call a new package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "c = [1; 1; 1]\n",
    "d = dot(a,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a'*c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Outside LinearAlgebra, the \"dot\" function will not work. This is how you add a package if you are working directly in the Julia REPL (The Julia session running when you write your code right into the computer's terminal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"LinearAlgebra\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is also very simple to define a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x,y)\n",
    "    return 3x + 2y\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Evaluate the function at (2,3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f(2,3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Isn't this simple? It can get even simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x,y) = 2x + 3y \n",
    "f(3,2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "That's enough to run our first algorithm. First, let us do it \"by hand\", then we are going to see how the Optimization Packages make our lifes much easier. Let us code a simple gradient descent for a function in \"R2\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Gradient Descent \n",
    "using LinearAlgebra\n",
    "cur_point = [-1.8;-1.6] #Starting Point\n",
    "alpha = 0.01 #\"Learning rate for the ML fans\" BTW, this is how we comment in Julia: #\n",
    "precision = 0.000000001\n",
    "previous_step_size = 1.0\n",
    "x_list = zeros(0)\n",
    "y_list = zeros(0)\n",
    "\n",
    "df(w) = 2*(w-1) #The gradient component of our function, we define this as a function, of course\n",
    "\n",
    "while previous_step_size > precision #While loop, simple as well\n",
    "    append!(x_list, cur_point[1])\n",
    "    append!(y_list, cur_point[2])\n",
    "    prev_point = cur_point\n",
    "    cur_point -= alpha*df.(prev_point) #Here we need to include the dot \".\" after the function name in order to evaluate it for a vector\n",
    "    previous_step_size = norm(cur_point - prev_point)\n",
    "    #println(\"Step size $previous_step_size\")\n",
    "end\n",
    "\n",
    "println(\"The local minimum occurs at $cur_point\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How about plotting the above? (Julia also accepts plotting packages that are familiar to Python programmers)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, let us plot the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Plots #This is Julia's basic Plot package\n",
    "x=range(-3,stop=3,length=100) #This is equivalent to linspace in Python (Self-explanatory).\n",
    "y=range(-3,stop=3,length=100)\n",
    "f(x,y) = (x-1)^2 + (y-1)^2 + 2\n",
    "plot(x,y,f,st=:surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "x=range(-2,stop=2,length=100)\n",
    "y=range(-2,stop=2,length=100)\n",
    "f(x,y) = (x-1)^2 + (y-1)^2 + 2\n",
    "plot(x,y,f,st=:contour) #Here the argument of st= is the only modification we make\n",
    "plot!(x_list, y_list, marker =2, ylabel = \"Y\", xlabel = \"X\", label = \"Trajectory\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although we can create nice graphs when we implement the algorithm \"line by line\", most real problems are much more complex and involve non trivial functions, different than the one above. Now that we are comfortable navigating through this new Julia world, let us finally talk about the Optimization Packages. The problem above is a non constrained optimization problem, that is, we're minimizing f(x) and x is not resricted to lie in a determined set of the domain of f (this is what happens when we're minimimazing cost functions in Machine Learning). What packages can we use to solve the problem above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g(x) = (x[1]-1)^2 + (x[2]-1)^2 + 2 #We must write x in the argument and enter the variables as x[1], x[2] instead of x,y,...\n",
    "using Optim\n",
    "opt = Optim.optimize(g,[-10,-10.1])\n",
    "println(\"optimal x =\", opt.minimizer)\n",
    "println(\"optimal f =\", opt.minimum)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, some constrained Optimization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"JuMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt #InteriorPointOPT\n",
    "\n",
    "my_model = Model(Ipopt.Optimizer)\n",
    "\n",
    "@variable(my_model, x[1:2])\n",
    "@NLobjective(my_model, Min, (x[1]-1)^2 + (x[2]-1)^2 + 2)\n",
    "@NLconstraint(my_model, x[1] + x[2] >= 0.5)\n",
    "@NLconstraint(my_model, x[1] + x[2] <= 0.75)\n",
    "\n",
    "JuMP.optimize!(my_model)\n",
    "println(\"** Opt obj func = \", JuMP.objective_value(my_model))\n",
    "println(\"** Opt sol = \", JuMP.value.(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\18143\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
